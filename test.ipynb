{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "from fl_preprocessing import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from fl_model import get_model\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "N_FEATURES = 11\n",
    "INPUT_DIM = N_FEATURES #check，此处特征值的设定不一定合理\n",
    "OUTPUT_DIM = 1\n",
    "HIDDEN_DIM = 64\n",
    "LAYER_DIM = 3\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.2\n",
    "EPOCH = 50\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "# FL Settings  训练7轮，客户端数目为5\n",
    "ROUND = 7\n",
    "NUM_CLIENTS = 2\n",
    "#from myconstants import *  #本篇中的所有常量引用来源\n",
    "# Models chosen from rnn, lstm #定义了LSTM模型的基本结构\n",
    "MODEL = \"lstm\"\n",
    "MODEL_PARAMS = {\"input_dim\": INPUT_DIM,\n",
    "                \"hidden_dim\": HIDDEN_DIM,\n",
    "                \"layer_dim\": LAYER_DIM,\n",
    "                \"output_dim\": OUTPUT_DIM,\n",
    "                \"dropout_prob\": DROPOUT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Transfused'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/anaconda3/envs/flower/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/anaconda3/envs/flower/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/anaconda3/envs/flower/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Transfused'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfl_preprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing_centralized\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpreprocessing_centralized\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m102.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1162.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/学习内容课程/研一/每周组会/实验部分FL/quickstart-sklearn-tabular/EVCDP-FL/fl_preprocessing.py:75\u001b[0m, in \u001b[0;36mpreprocessing_centralized\u001b[0;34m(filepaths)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_n\n\u001b[1;32m     73\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m N_FEATURES \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 75\u001b[0m df_generated \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_time_lags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m df_features \u001b[38;5;241m=\u001b[39m ( df_generated\n\u001b[1;32m     78\u001b[0m                 \u001b[38;5;241m.\u001b[39massign(day \u001b[38;5;241m=\u001b[39m df_generated\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mday)\n\u001b[1;32m     79\u001b[0m                 \u001b[38;5;241m.\u001b[39massign(month \u001b[38;5;241m=\u001b[39m df_generated\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmonth)\n\u001b[1;32m     80\u001b[0m                 \u001b[38;5;241m.\u001b[39massign(day_of_week \u001b[38;5;241m=\u001b[39m df_generated\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mdayofweek)\n\u001b[1;32m     81\u001b[0m                 \u001b[38;5;241m.\u001b[39massign(week_of_year \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mIndex(df_generated\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misocalendar()\u001b[38;5;241m.\u001b[39mweek)))\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# def onehot_encode_pd(df, cols):\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#     for col in cols:\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#         dummies = pd.get_dummies(df[col], prefix=col)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# df_features = generate_cyclical_features(df_features, 'day_of_week', 7, 0)\u001b[39;00m\n",
      "File \u001b[0;32m~/学习内容课程/研一/每周组会/实验部分FL/quickstart-sklearn-tabular/EVCDP-FL/fl_preprocessing.py:69\u001b[0m, in \u001b[0;36mpreprocessing_centralized.<locals>.generate_time_lags\u001b[0;34m(df, n_lags)\u001b[0m\n\u001b[1;32m     67\u001b[0m df_n \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_lags \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 69\u001b[0m     df_n[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlag\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_n\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTransfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshift(n)\n\u001b[1;32m     70\u001b[0m df_n \u001b[38;5;241m=\u001b[39m df_n\u001b[38;5;241m.\u001b[39miloc[n_lags:]\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_n\n",
      "File \u001b[0;32m/Library/anaconda3/envs/flower/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/anaconda3/envs/flower/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Transfused'"
     ]
    }
   ],
   "source": [
    "from fl_preprocessing import preprocessing_centralized\n",
    "preprocessing_centralized(['102.csv','1162.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepaths=['102.csv','1162.csv']\n",
    "df = pd.read_csv(filepaths[0])\n",
    "for i in range(1, len(filepaths)):\n",
    "    df2 = pd.read_csv(filepaths[i])\n",
    "    df._append(df2, ignore_index = True)\n",
    "df.columns=['Datetime', 'Occupancy']\n",
    "df.set_index('Datetime',inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "def generate_time_lags(df, n_lags):\n",
    "    df_n = df.copy()\n",
    "    for n in range(1, n_lags + 1):\n",
    "        df_n[f\"lag{n}\"] = df_n[\"Occupancy\"].shift(n)\n",
    "    df_n = df_n.iloc[n_lags:]\n",
    "    return df_n\n",
    "input_dim = 5\n",
    "\n",
    "df_generated = generate_time_lags(df, input_dim)\n",
    "\n",
    "df_features = ( df_generated\n",
    "                .assign(day = df_generated.index.day)\n",
    "                .assign(month = df_generated.index.month)\n",
    "                .assign(day_of_week = df_generated.index.dayofweek)\n",
    "                .assign(week_of_year = pd.Index(df_generated.index.isocalendar().week)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>lag1</th>\n",
       "      <th>lag2</th>\n",
       "      <th>lag3</th>\n",
       "      <th>lag4</th>\n",
       "      <th>lag5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week_of_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-06-19 00:25:00</th>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-19 00:30:00</th>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-19 00:35:00</th>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-19 00:40:00</th>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-19 00:45:00</th>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18 23:35:00</th>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18 23:40:00</th>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18 23:45:00</th>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18 23:50:00</th>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18 23:55:00</th>\n",
       "      <td>16</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8635 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Occupancy  lag1  lag2  ...  month  day_of_week  week_of_year\n",
       "Datetime                                    ...                                  \n",
       "2022-06-19 00:25:00         12  12.0  12.0  ...      6            6            24\n",
       "2022-06-19 00:30:00         12  12.0  12.0  ...      6            6            24\n",
       "2022-06-19 00:35:00         12  12.0  12.0  ...      6            6            24\n",
       "2022-06-19 00:40:00         12  12.0  12.0  ...      6            6            24\n",
       "2022-06-19 00:45:00         12  12.0  12.0  ...      6            6            24\n",
       "...                        ...   ...   ...  ...    ...          ...           ...\n",
       "2022-07-18 23:35:00         18  18.0  18.0  ...      7            0            29\n",
       "2022-07-18 23:40:00         18  18.0  18.0  ...      7            0            29\n",
       "2022-07-18 23:45:00         18  18.0  18.0  ...      7            0            29\n",
       "2022-07-18 23:50:00         18  18.0  18.0  ...      7            0            29\n",
       "2022-07-18 23:55:00         16  18.0  18.0  ...      7            0            29\n",
       "\n",
       "[8635 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Occupancy  lag1  lag2  lag3  lag4  lag5\n",
      "Datetime                                                    \n",
      "2022-06-19 00:25:00         12  12.0  12.0  12.0  12.0  12.0\n",
      "2022-06-19 00:30:00         12  12.0  12.0  12.0  12.0  12.0\n",
      "2022-06-19 00:35:00         12  12.0  12.0  12.0  12.0  12.0\n",
      "2022-06-19 00:40:00         12  12.0  12.0  12.0  12.0  12.0\n",
      "2022-06-19 00:45:00         12  12.0  12.0  12.0  12.0  12.0\n",
      "...                        ...   ...   ...   ...   ...   ...\n",
      "2022-07-18 23:35:00         18  18.0  18.0  18.0  18.0  18.0\n",
      "2022-07-18 23:40:00         18  18.0  18.0  18.0  18.0  18.0\n",
      "2022-07-18 23:45:00         18  18.0  18.0  18.0  18.0  18.0\n",
      "2022-07-18 23:50:00         18  18.0  18.0  18.0  18.0  18.0\n",
      "2022-07-18 23:55:00         16  18.0  18.0  18.0  18.0  18.0\n",
      "\n",
      "[8635 rows x 6 columns]\n",
      "                     Occupancy  lag1  lag2  lag3  lag4  lag5\n",
      "Datetime                                                    \n",
      "2022-06-19 00:25:00         12  12.0  12.0  12.0  12.0  12.0\n",
      "2022-06-19 00:30:00         12  12.0  12.0  12.0  12.0  12.0\n",
      "2022-06-19 00:35:00         12  12.0  12.0  12.0  12.0  12.0\n",
      "2022-06-19 00:40:00         13  12.0  12.0  12.0  12.0  12.0\n",
      "2022-06-19 00:45:00         13  13.0  12.0  12.0  12.0  12.0\n",
      "...                        ...   ...   ...   ...   ...   ...\n",
      "2022-07-18 23:35:00         10  10.0   9.0   9.0   9.0   9.0\n",
      "2022-07-18 23:40:00         11  10.0  10.0   9.0   9.0   9.0\n",
      "2022-07-18 23:45:00         11  11.0  10.0  10.0   9.0   9.0\n",
      "2022-07-18 23:50:00         11  11.0  11.0  10.0  10.0   9.0\n",
      "2022-07-18 23:55:00         11  11.0  11.0  11.0  10.0  10.0\n",
      "\n",
      "[8635 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "def load_data(batch_size: int):\n",
    "    datasets = [\"102.csv\", \"1162.csv\"]\n",
    "    train_loaders = []\n",
    "    test_loaders = []\n",
    "    nums_examples = []\n",
    "    nums_features = []\n",
    "    X_tests = []\n",
    "    scalers = []\n",
    "\n",
    "    for path in datasets:\n",
    "        X_train_arr, X_test_arr, y_train_arr, y_test_arr, X_test, scaler = preprocessing(path)\n",
    "        #定义预处理函数preprocessing\n",
    "        train_features = torch.Tensor(X_train_arr).to(DEVICE)\n",
    "        train_targets = torch.Tensor(y_train_arr).to(DEVICE)\n",
    "\n",
    "        test_features = torch.Tensor(X_test_arr).to(DEVICE)\n",
    "        test_targets = torch.Tensor(y_test_arr).to(DEVICE)\n",
    "\n",
    "        train = TensorDataset(train_features, train_targets)\n",
    "        test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "        train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        num_examples = {\"trainset\": len(X_train_arr), \"testset\":len(X_test_arr)}\n",
    "        num_features = X_train_arr.shape[1]\n",
    "\n",
    "        train_loaders.append(train_loader)\n",
    "        test_loaders.append(test_loader)\n",
    "        nums_examples.append(num_examples)\n",
    "        nums_features.append(num_features)\n",
    "        X_tests.append(X_test)\n",
    "        scalers.append(scaler)\n",
    "    return train_loaders, test_loader, nums_examples, nums_features, X_tests, scalers\n",
    "\n",
    "def train(net, train_loader, epochs):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def train_step(x, y):\n",
    "        net.train()\n",
    "        yhat = net(x) #make prediction\n",
    "        loss = loss_fn(y, yhat) #compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.view([BATCH_SIZE, -1, N_FEATURES]).to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "            loss = train_step(x_batch, y_batch)\n",
    "\n",
    "def test(net, testloader, X_test, scaler):\n",
    "    loss = 0\n",
    "    criteron = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        values = []\n",
    "        for x_test, y_test in testloader:\n",
    "            x_test = x_test.view([BATCH_SIZE, -1, N_FEATURES]).to(DEVICE)\n",
    "            y_test=y_test.to(DEVICE)\n",
    "            net.eval()\n",
    "            yhat = net(x_test)\n",
    "            predictions.append(yhat.cpu().numpy())\n",
    "            values.append(y_test.cpu().numpy())\n",
    "            loss += criteron(yhat, y_test)\n",
    "\n",
    "    df_result = format_predictions(predictions, values, X_test, scaler)\n",
    "    rmse = mean_squared_error(df_result.value, df_result.prediction, squared=False)\n",
    "    return loss, rmse\n",
    "\n",
    "def inverse_transform(scaler, df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = scaler.inverse_transform(df[col])\n",
    "    return df\n",
    "\n",
    "def format_predictions(predictions, values, df_test, scaler):\n",
    "    vals = np.concatenate(values, axis=0).ravel()\n",
    "    preds = np.concatenate(predictions, axis=0).ravel()\n",
    "    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds}, index=df_test.head(len(vals)).index)\n",
    "    df_result = df_result.sort_index()\n",
    "    df_result = inverse_transform(scaler, df_result, [[\"value\", \"prediction\"]])\n",
    "    return df_result\n",
    "\n",
    "trainloaders, testloaders, nums_examples, nums_features, X_tests, scalers = load_data(batch_size=BATCH_SIZE)\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fdde875ca60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, testloader, num_examples, num_features, X_test, scaler):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "        self.num_examples = num_examples\n",
    "        self.num_features = num_features\n",
    "        self.X_test = X_test\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=EPOCH)\n",
    "        return self.get_parameters(config={}), self.num_examples[\"trainset\"], {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, rmse = test(self.net, self.testloader, self.X_test, self.scaler)\n",
    "        print(\"loss \", loss)\n",
    "        print(\"rmse \", rmse)\n",
    "        return float(loss), self.num_examples[\"testset\"], {\"rmse\": float(rmse)}\n",
    "\n",
    "def client_fn(cid) -> Client:  #client fn需要定义训练模型使用的网络，数据loader\n",
    "    net = get_model(MODEL, MODEL_PARAMS).to(DEVICE)  #定义将要使用的模型\n",
    "    trainloader = trainloaders[int(cid)]   #训练数据加载\n",
    "    testloader = testloaders[int(cid)]   #测试数据加载\n",
    "    num_examples = nums_examples[int(cid)]   #实例数\n",
    "    num_features = nums_features[int(cid)]   #特征数\n",
    "    X_test = X_tests[int(cid)]   #from load_data\n",
    "    scaler = scalers[int(cid)]   #from load_data\n",
    "    #返回值是flower定义好的client类\n",
    "    return Client(cid, net, trainloader, testloader, num_examples, num_features, X_test, scaler)  #return CLient Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_resources = None \n",
    "if DEVICE.type == \"cuda\":   \n",
    "  client_resources = {\"num_gpus\": 1}\n",
    "\n",
    "# FedAVG/FedProx algorithm\n",
    "class CustomStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "        if aggregated_parameters is not None:\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "            # Save aggregated_ndarrays\n",
    "            print(f\"Saving round {server_round} aggregated_ndarrays...\")\n",
    "            np.savez(f\"./flower/savedmodels/round-{server_round}-weights.npz\", *aggregated_ndarrays)\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics\n",
    "\n",
    "    def aggregate_evaluate(self, server_round, results, failures):\n",
    "        \"\"\"Aggregate evaluation rmse using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        # Call aggregate_evaluate from base class (FedAvg) to aggregate loss and metrics\n",
    "        aggregated_loss, aggregated_metrics = super().aggregate_evaluate(server_round, results, failures)\n",
    "\n",
    "        # Weigh rmse of each client by number of examples used\n",
    "        rmses = [r.metrics[\"rmse\"] * r.num_examples for _, r in results]\n",
    "        examples = [r.num_examples for _, r in results]\n",
    "\n",
    "        # Aggregate and print custom metric\n",
    "        aggregated_rmse = sum(rmses) / sum(examples)\n",
    "        print(f\"Round {server_round} rmse aggregated from client results: {aggregated_rmse}\")\n",
    "\n",
    "        # Return aggregated loss and metrics (i.e., aggregated rmse)\n",
    "        return aggregated_loss, {\"rmse\": aggregated_rmse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # strategy = CustomStrategy(proximal_mu=1)\n",
    "    strategy = CustomStrategy()\n",
    "\n",
    "    fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        config=fl.server.ServerConfig(num_rounds=ROUND),\n",
    "        client_resources=client_resources,\n",
    "        strategy = strategy,\n",
    "        ray_init_args = {\"include_dashboard\": False}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4., 5., 6.]])\n",
      "tensor([[1., 2., 3., 4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.Tensor([[[1,2,3],[4,5,6]]])\n",
    "b=torch.Tensor([1,2,3,4,5,6])\n",
    "\n",
    "print(a.view(1,6))\n",
    "print(b.view(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
